{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44590bb828b674f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T15:22:56.467963735Z",
     "start_time": "2023-09-13T15:22:55.863158165Z"
    }
   },
   "outputs": [],
   "source": [
    "# selenium and chromium must be pre-installed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import traceback\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from fake_useragent import UserAgent\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5b22f",
   "metadata": {},
   "source": [
    "### RBC.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859e390f2a03c851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T15:22:56.486833318Z",
     "start_time": "2023-09-13T15:22:56.473899311Z"
    }
   },
   "outputs": [],
   "source": [
    "urls = [\"https://www.rbc.ru/finances/?utm_source=topline\", \"https://www.rbc.ru/economics/?utm_source=topline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1131250e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T15:22:58.894769970Z",
     "start_time": "2023-09-13T15:22:56.954973999Z"
    }
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ba2b7dc77605fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T15:23:16.700650789Z",
     "start_time": "2023-09-13T15:23:16.687866161Z"
    }
   },
   "outputs": [],
   "source": [
    "class Scrapper:\n",
    "    \"\"\"\n",
    "    Collecting pages urls and texts from rbc.ru\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url, max_volume, save_path):\n",
    "        self.url = base_url\n",
    "        self.max_vol = max_volume\n",
    "        self.path = save_path\n",
    "        self.dtf = pd.DataFrame(columns=[\n",
    "            \"url\"\n",
    "        ])\n",
    "    \n",
    "    def get_urls(self):\n",
    "        num = self.max_vol\n",
    "        \n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(self.url)\n",
    "        \n",
    "        try:\n",
    "            popup = driver.find_element(By.CLASS_NAME, \"live-tv-popup__close\")\n",
    "            popup.click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        urls = []\n",
    "        \n",
    "        while num:\n",
    "            try:\n",
    "                objects = driver.find_elements(By.CLASS_NAME, 'item__link')\n",
    "                url = []\n",
    "                for obj in objects:\n",
    "                    urls.append(obj.get_attribute(\"href\"))\n",
    "                \n",
    "                num -= 1\n",
    "            except Exception as er:\n",
    "                print(er)\n",
    "                driver.quit()\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            print(f\"INFO collected {len(urls)} urls\")\n",
    "            sleep(9)\n",
    "        \n",
    "        self.dtf = pd.concat([self.dtf, pd.DataFrame(urls, columns=[\"url\"])], ignore_index=True)\n",
    "        \n",
    "        driver.quit()\n",
    "        self.save()\n",
    "        \n",
    "    def extract(self):\n",
    "        urls = pd.read_csv(self.path)\n",
    "        \n",
    "        driver = webdriver.Chrome()\n",
    "        texts = []\n",
    "        urls['text'] = None\n",
    "            \n",
    "        for i in range(urls.shape[0]):\n",
    "            driver.get(urls.loc[i][\"url\"])\n",
    "        \n",
    "            try:\n",
    "                popup = driver.find_element(By.CLASS_NAME, \"live-tv-popup__close\")\n",
    "                popup.click()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                text = ''\n",
    "                objects = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "                for obj in objects:\n",
    "                    text += obj.text\n",
    "                urls[\"text\"][i] = text\n",
    "                \n",
    "            except:\n",
    "                urls.to_csv(self.path)\n",
    "                driver.quit()\n",
    "            sleep(2)\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(f\"Collected {i} news\")\n",
    "        \n",
    "        urls.to_csv(self.path)\n",
    "        driver.quit()\n",
    "    \n",
    "    def save(self):\n",
    "        self.dtf.to_csv(self.path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f719ec2ce326626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T15:27:50.648130751Z",
     "start_time": "2023-09-13T15:23:21.569152741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO collected 20 urls\n",
      "INFO collected 52 urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22771/2849467971.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  urls[\"text\"][i] = text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 0 news\n",
      "Collected 50 news\n"
     ]
    }
   ],
   "source": [
    "sc = Scrapper(\"https://www.rbc.ru/finances/?utm_source=topline\", max_volume=2, save_path=\"test.csv\")\n",
    "sc.get_urls()\n",
    "sc.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c8308578b9aec109"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb716b52a2142b7",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-09-13T15:28:25.872648781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO collected 20 urls\n",
      "INFO collected 52 urls\n",
      "INFO collected 96 urls\n",
      "INFO collected 152 urls\n",
      "INFO collected 220 urls\n",
      "INFO collected 300 urls\n",
      "INFO collected 380 urls\n",
      "INFO collected 460 urls\n",
      "INFO collected 540 urls\n",
      "INFO collected 620 urls\n",
      "INFO collected 700 urls\n",
      "INFO collected 780 urls\n",
      "INFO collected 860 urls\n",
      "INFO collected 940 urls\n",
      "INFO collected 1020 urls\n",
      "INFO collected 1100 urls\n",
      "INFO collected 1180 urls\n",
      "INFO collected 1260 urls\n",
      "INFO collected 1340 urls\n",
      "INFO collected 1420 urls\n",
      "INFO collected 1500 urls\n",
      "INFO collected 1580 urls\n",
      "INFO collected 1660 urls\n",
      "INFO collected 1740 urls\n",
      "INFO collected 1820 urls\n",
      "INFO collected 1900 urls\n",
      "INFO collected 1980 urls\n",
      "INFO collected 2060 urls\n",
      "INFO collected 2140 urls\n",
      "INFO collected 2220 urls\n",
      "INFO collected 2300 urls\n",
      "INFO collected 2380 urls\n",
      "INFO collected 2460 urls\n",
      "INFO collected 2540 urls\n",
      "INFO collected 2620 urls\n",
      "INFO collected 2700 urls\n",
      "INFO collected 2780 urls\n",
      "INFO collected 2860 urls\n",
      "INFO collected 2940 urls\n",
      "INFO collected 3020 urls\n",
      "INFO collected 3112 urls\n",
      "INFO collected 3216 urls\n",
      "INFO collected 3320 urls\n",
      "INFO collected 3424 urls\n",
      "INFO collected 3528 urls\n",
      "INFO collected 3632 urls\n",
      "INFO collected 3736 urls\n",
      "INFO collected 3840 urls\n",
      "INFO collected 3944 urls\n",
      "INFO collected 4048 urls\n",
      "INFO collected 4152 urls\n",
      "INFO collected 4256 urls\n",
      "INFO collected 4360 urls\n",
      "INFO collected 4464 urls\n",
      "INFO collected 4568 urls\n",
      "INFO collected 4672 urls\n",
      "INFO collected 4776 urls\n",
      "INFO collected 4880 urls\n",
      "INFO collected 4984 urls\n",
      "INFO collected 5088 urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22771/2849467971.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  urls[\"text\"][i] = text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 0 news\n"
     ]
    }
   ],
   "source": [
    "fin = Scrapper(urls[0], max_volume=60, save_path=\"finance.scv\")\n",
    "fin.get_urls()\n",
    "fin.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae815c76eb1d19a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bis = Scrapper(urls[1], max_volume=60, save_path=\"business.csv\")\n",
    "bis.get_urls()\n",
    "bis.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc99a4e5c244d4",
   "metadata": {},
   "source": [
    "### CyberLeninka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "311f9de74c7b6cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T10:21:50.334678388Z",
     "start_time": "2023-09-13T10:21:50.285955830Z"
    }
   },
   "outputs": [],
   "source": [
    "class CyberScrapper:\n",
    "    \"\"\"\n",
    "    Collecting pages urls and texts from cyberleninka.ru\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url, max_volume, save_path, num_page=None):\n",
    "        self.url = base_url\n",
    "        self.max_vol = max_volume\n",
    "        self.path = save_path\n",
    "        self.num_page = num_page\n",
    "        self.columns = [\"url\", \"author\", \"title\", \"text\", \"year\", \"labels\", \"views\", \n",
    "                                          \"downloads\", \"likes\", \"dislikes\", \"journal\"]\n",
    "        self.data = pd.DataFrame(columns=self.columns)\n",
    "        \n",
    "    def get(self):\n",
    "        \n",
    "        ua = UserAgent()\n",
    "        user_agent = ua.random\n",
    "        \n",
    "        driver = webdriver.Chrome()\n",
    "        \n",
    "        driver.execute_cdp_cmd(\"Network.setUserAgentOverride\", {\"userAgent\": user_agent})\n",
    "        if self.num_page:\n",
    "            driver.get(self.url + f\"/{self.num_page}\")\n",
    "        else:\n",
    "            driver.get(self.url)\n",
    "        num = self.max_vol\n",
    "        \n",
    "        # num of li elements on the page\n",
    "        last_paper_on_page = -7\n",
    "        if self.num_page:\n",
    "            page_num = self.num_page\n",
    "        else:\n",
    "            page_num = 2\n",
    "        \n",
    "        try:\n",
    "            while num:\n",
    "                \n",
    "                print(f\"Papers {self.data.shape[0]} saved\")\n",
    "                elements = driver.find_elements(By.TAG_NAME, \"li\")\n",
    "                articles = elements[:last_paper_on_page]\n",
    "                next_page = self.url + f\"/{page_num}\"\n",
    "                \n",
    "                for article in articles:\n",
    "                    \n",
    "                    num -= 1\n",
    "                    href = article.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\") \n",
    "                    driver.get(href)\n",
    "                    \n",
    "                    objects = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "                    \n",
    "                    # get text of paper\n",
    "                    text = ''\n",
    "                    for obj in objects:\n",
    "                        text += obj.text\n",
    "                    \n",
    "                    # author\n",
    "                    try:\n",
    "                        author = driver.find_element(By.CLASS_NAME, \"hl\").text\n",
    "                    except:\n",
    "                        author = None\n",
    "                    try:\n",
    "                        views = driver.find_element(By.CLASS_NAME, \"statitem.views\").text\n",
    "                    except:\n",
    "                        views = None\n",
    "                    try:\n",
    "                        down = driver.find_element(By.CLASS_NAME, \"statitem.downloads\").text\n",
    "                    except:\n",
    "                        down = None\n",
    "                    try:\n",
    "                        likes = driver.find_element(By.CLASS_NAME, \"likes\").text.split(\"\\n\")\n",
    "                    except:\n",
    "                        likes = [None, None]\n",
    "                    try:\n",
    "                        year = driver.find_element(By.CLASS_NAME, \"label.year\").find_element(By.TAG_NAME, \"time\").text\n",
    "                    except:\n",
    "                        year = None\n",
    "                    try:    \n",
    "                        journal = driver.find_element(By.CLASS_NAME, \"half\").find_elements(By.TAG_NAME, \"a\")[-1].text\n",
    "                    except:\n",
    "                        journal = None\n",
    "                    try:\n",
    "                        words = [i.text for i in driver.find_element(By.CLASS_NAME, \"full.keywords\").find_elements(By.CLASS_NAME, \"hl.to-search\")]\n",
    "                    except:\n",
    "                        words = None\n",
    "                    try:\n",
    "                        title = driver.find_element(By.TAG_NAME, \"i\").text\n",
    "                    except:\n",
    "                        title = None\n",
    "                    \n",
    "                    lst = [(href, \n",
    "                            author, \n",
    "                            title,\n",
    "                            text, \n",
    "                            year, \n",
    "                            words, \n",
    "                            views, \n",
    "                            down, \n",
    "                            likes[0], \n",
    "                            likes[1], \n",
    "                            journal)]\n",
    "                    to_add = pd.DataFrame(lst, columns=self.columns)\n",
    "                    self.data = pd.concat([self.data, to_add], ignore_index=True)\n",
    "                    sleep(10)\n",
    "                    driver.back()\n",
    "                \n",
    "                # Change UA \n",
    "                driver.execute_cdp_cmd(\"Network.setUserAgentOverride\", {\"userAgent\": ua.random})\n",
    "                driver.get(next_page)\n",
    "                page_num += 1\n",
    "                \n",
    "        except Exception as ex:\n",
    "            print(\"Last page:\", page_num)\n",
    "            traceback.print_exc()\n",
    "            driver.quit()\n",
    "            \n",
    "        print(\"Last page:\", page_num)\n",
    "        return self.data\n",
    "    \n",
    "    def save(self):\n",
    "        self.data.to_csv(self.path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "126f7a9b65dd8e64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:02:39.323930558Z",
     "start_time": "2023-09-13T10:22:01.671789457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers 0 saved\n",
      "Papers 20 saved\n",
      "Papers 40 saved\n",
      "Papers 60 saved\n",
      "Papers 80 saved\n",
      "Papers 100 saved\n",
      "Papers 120 saved\n",
      "Papers 140 saved\n",
      "Papers 160 saved\n",
      "Papers 180 saved\n",
      "Papers 200 saved\n",
      "Papers 220 saved\n",
      "Papers 240 saved\n",
      "Papers 260 saved\n",
      "Papers 280 saved\n",
      "Papers 300 saved\n",
      "Papers 320 saved\n",
      "Papers 340 saved\n",
      "Papers 360 saved\n",
      "Papers 380 saved\n",
      "Papers 400 saved\n",
      "Papers 420 saved\n",
      "Papers 440 saved\n",
      "Papers 460 saved\n",
      "Last page: 6666\n",
      "Last page: 6666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6009/3340503310.py\", line 46, in get\n",
      "    href = article.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
      "  File \"/home/igor/projects/data_wsd/data_wsd/lib/python3.10/site-packages/selenium/webdriver/remote/webelement.py\", line 416, in find_element\n",
      "    return self._execute(Command.FIND_CHILD_ELEMENT, {\"using\": by, \"value\": value})[\"value\"]\n",
      "  File \"/home/igor/projects/data_wsd/data_wsd/lib/python3.10/site-packages/selenium/webdriver/remote/webelement.py\", line 394, in _execute\n",
      "    return self._parent.execute(command, params)\n",
      "  File \"/home/igor/projects/data_wsd/data_wsd/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py\", line 344, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"/home/igor/projects/data_wsd/data_wsd/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py\", line 229, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.StaleElementReferenceException: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=117.0.5938.62); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "#0 0x55d471506693 <unknown>\n",
      "#1 0x55d4711dc1e7 <unknown>\n",
      "#2 0x55d4711e9f09 <unknown>\n",
      "#3 0x55d4711e1644 <unknown>\n",
      "#4 0x55d4711e025b <unknown>\n",
      "#5 0x55d4711e29c0 <unknown>\n",
      "#6 0x55d4711e2a7c <unknown>\n",
      "#7 0x55d471223296 <unknown>\n",
      "#8 0x55d471223651 <unknown>\n",
      "#9 0x55d471219046 <unknown>\n",
      "#10 0x55d4712450ed <unknown>\n",
      "#11 0x55d471218f16 <unknown>\n",
      "#12 0x55d47124528e <unknown>\n",
      "#13 0x55d47125e322 <unknown>\n",
      "#14 0x55d471244e93 <unknown>\n",
      "#15 0x55d471217934 <unknown>\n",
      "#16 0x55d47121871e <unknown>\n",
      "#17 0x55d4714cbc98 <unknown>\n",
      "#18 0x55d4714cfbd0 <unknown>\n",
      "#19 0x55d4714da17c <unknown>\n",
      "#20 0x55d4714d07e8 <unknown>\n",
      "#21 0x55d47149d25f <unknown>\n",
      "#22 0x55d4714f4e68 <unknown>\n",
      "#23 0x55d4714f5039 <unknown>\n",
      "#24 0x55d471505823 <unknown>\n",
      "#25 0x7f0ed2c94b43 <unknown>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                   url  \\\n0    https://cyberleninka.ru/article/n/analiz-effek...   \n1    https://cyberleninka.ru/article/n/metodicheski...   \n2    https://cyberleninka.ru/article/n/puti-optimiz...   \n3    https://cyberleninka.ru/article/n/perspektivy-...   \n4    https://cyberleninka.ru/article/n/setevye-medi...   \n..                                                 ...   \n456  https://cyberleninka.ru/article/n/odin-iz-aspe...   \n457  https://cyberleninka.ru/article/n/perspektivy-...   \n458  https://cyberleninka.ru/article/n/povyshenie-e...   \n459  https://cyberleninka.ru/article/n/rasshirenie-...   \n460  https://cyberleninka.ru/article/n/kontseptualn...   \n\n                             author  \\\n0                       Гришин А.В.   \n1                       Петров А.Г.   \n2                    Баранкина Т.А.   \n3                     Воробьёв В.М.   \n4    Бобровский Андрей Вениаминович   \n..                              ...   \n456        Ерзин Олег Александрович   \n457       Белянская Елена Сергеевна   \n458           Апина Анна Михайловна   \n459   Козельский Алексей Викторович   \n460    Чумаченко Наталья Эдуардовна   \n\n                                                 title  \\\n0    Анализ эффективности применения поощрений для ...   \n1    Методические подходы к прогнозированию позитив...   \n2    Пути оптимизации лекарственного обеспечения в ...   \n3    Перспективы адсорбирующей повязки на основе на...   \n4    Сетевые медицинские организации: стратегия раз...   \n..                                                 ...   \n456  Один из аспектов оценки эффективности технолог...   \n457  Перспективы использования электронно-библиотеч...   \n458  Повышение эффективности труда персонала на осн...   \n459  Расширение сферы инфраструктурных услуг в проц...   \n460  Концептуальные подходы к исследованию отраслев...   \n\n                                                  text  year  \\\n0    Представлены анализ состояния организации немо...  2010   \n1    Представлены методические подходы к прогнозиро...  2010   \n2    С целью оптимизации лекарственного обеспечения...  2010   \n3    Проведено маркетинговое исследование по перспе...  2010   \n4    Осознание преимуществ сетевых решений в здраво...  2010   \n..                                                 ...   ...   \n456  В работе предложен энергетический подход к оце...  2014   \n457  Проанализированы перспективы использования эле...  2014   \n458  В статье приведена характеристика проблем повы...  2014   \n459  В статье обосновывается, что расширение сферы ...  2014   \n460  Финансово-экономический кризис, совпавший с по...  2014   \n\n                                                labels views downloads likes  \\\n0    [мотивация, поощрения, немонетарное стимулиров...  1410       217     0   \n1    [прогнозирование риска, имидж, фармацевтическа...   268        64     0   \n2    [оптимизация, лекарственное обеспечение, эффек...    71        19     0   \n3    [перевязочные средства, адсорбирующая повязка,...   435        77     0   \n4    [сетевые медицинские организации, менеджмент, ...   515        61     0   \n..                                                 ...   ...       ...   ...   \n456  [технологическая система, эффективность исполь...   173       303     0   \n457  [электронно-библиотечная система, электронные ...   233       129     0   \n458  [повышение эффективности труда персонала, инве...  1354       160     0   \n459  [инфраструктура национальной экономики, инфрас...   471        59     0   \n460  [большие волны конъюнктуры н.д. кондратьева, ц...   367       100     0   \n\n    dislikes                                            journal  \n0          0                       Бюллетень сибирской медицины  \n1          0                       Бюллетень сибирской медицины  \n2          0                       Бюллетень сибирской медицины  \n3          0                       Бюллетень сибирской медицины  \n4          0                       Бюллетень сибирской медицины  \n..       ...                                                ...  \n456        0  Известия Тульского государственного университе...  \n457        0  Известия Тульского государственного университе...  \n458        0  Промышленность: экономика, управление, технологии  \n459        0  Промышленность: экономика, управление, технологии  \n460        0  Промышленность: экономика, управление, технологии  \n\n[461 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>url</th>\n      <th>author</th>\n      <th>title</th>\n      <th>text</th>\n      <th>year</th>\n      <th>labels</th>\n      <th>views</th>\n      <th>downloads</th>\n      <th>likes</th>\n      <th>dislikes</th>\n      <th>journal</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://cyberleninka.ru/article/n/analiz-effek...</td>\n      <td>Гришин А.В.</td>\n      <td>Анализ эффективности применения поощрений для ...</td>\n      <td>Представлены анализ состояния организации немо...</td>\n      <td>2010</td>\n      <td>[мотивация, поощрения, немонетарное стимулиров...</td>\n      <td>1410</td>\n      <td>217</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Бюллетень сибирской медицины</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://cyberleninka.ru/article/n/metodicheski...</td>\n      <td>Петров А.Г.</td>\n      <td>Методические подходы к прогнозированию позитив...</td>\n      <td>Представлены методические подходы к прогнозиро...</td>\n      <td>2010</td>\n      <td>[прогнозирование риска, имидж, фармацевтическа...</td>\n      <td>268</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Бюллетень сибирской медицины</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://cyberleninka.ru/article/n/puti-optimiz...</td>\n      <td>Баранкина Т.А.</td>\n      <td>Пути оптимизации лекарственного обеспечения в ...</td>\n      <td>С целью оптимизации лекарственного обеспечения...</td>\n      <td>2010</td>\n      <td>[оптимизация, лекарственное обеспечение, эффек...</td>\n      <td>71</td>\n      <td>19</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Бюллетень сибирской медицины</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://cyberleninka.ru/article/n/perspektivy-...</td>\n      <td>Воробьёв В.М.</td>\n      <td>Перспективы адсорбирующей повязки на основе на...</td>\n      <td>Проведено маркетинговое исследование по перспе...</td>\n      <td>2010</td>\n      <td>[перевязочные средства, адсорбирующая повязка,...</td>\n      <td>435</td>\n      <td>77</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Бюллетень сибирской медицины</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://cyberleninka.ru/article/n/setevye-medi...</td>\n      <td>Бобровский Андрей Вениаминович</td>\n      <td>Сетевые медицинские организации: стратегия раз...</td>\n      <td>Осознание преимуществ сетевых решений в здраво...</td>\n      <td>2010</td>\n      <td>[сетевые медицинские организации, менеджмент, ...</td>\n      <td>515</td>\n      <td>61</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Бюллетень сибирской медицины</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>456</th>\n      <td>https://cyberleninka.ru/article/n/odin-iz-aspe...</td>\n      <td>Ерзин Олег Александрович</td>\n      <td>Один из аспектов оценки эффективности технолог...</td>\n      <td>В работе предложен энергетический подход к оце...</td>\n      <td>2014</td>\n      <td>[технологическая система, эффективность исполь...</td>\n      <td>173</td>\n      <td>303</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Известия Тульского государственного университе...</td>\n    </tr>\n    <tr>\n      <th>457</th>\n      <td>https://cyberleninka.ru/article/n/perspektivy-...</td>\n      <td>Белянская Елена Сергеевна</td>\n      <td>Перспективы использования электронно-библиотеч...</td>\n      <td>Проанализированы перспективы использования эле...</td>\n      <td>2014</td>\n      <td>[электронно-библиотечная система, электронные ...</td>\n      <td>233</td>\n      <td>129</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Известия Тульского государственного университе...</td>\n    </tr>\n    <tr>\n      <th>458</th>\n      <td>https://cyberleninka.ru/article/n/povyshenie-e...</td>\n      <td>Апина Анна Михайловна</td>\n      <td>Повышение эффективности труда персонала на осн...</td>\n      <td>В статье приведена характеристика проблем повы...</td>\n      <td>2014</td>\n      <td>[повышение эффективности труда персонала, инве...</td>\n      <td>1354</td>\n      <td>160</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Промышленность: экономика, управление, технологии</td>\n    </tr>\n    <tr>\n      <th>459</th>\n      <td>https://cyberleninka.ru/article/n/rasshirenie-...</td>\n      <td>Козельский Алексей Викторович</td>\n      <td>Расширение сферы инфраструктурных услуг в проц...</td>\n      <td>В статье обосновывается, что расширение сферы ...</td>\n      <td>2014</td>\n      <td>[инфраструктура национальной экономики, инфрас...</td>\n      <td>471</td>\n      <td>59</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Промышленность: экономика, управление, технологии</td>\n    </tr>\n    <tr>\n      <th>460</th>\n      <td>https://cyberleninka.ru/article/n/kontseptualn...</td>\n      <td>Чумаченко Наталья Эдуардовна</td>\n      <td>Концептуальные подходы к исследованию отраслев...</td>\n      <td>Финансово-экономический кризис, совпавший с по...</td>\n      <td>2014</td>\n      <td>[большие волны конъюнктуры н.д. кондратьева, ц...</td>\n      <td>367</td>\n      <td>100</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Промышленность: экономика, управление, технологии</td>\n    </tr>\n  </tbody>\n</table>\n<p>461 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://cyberleninka.ru/article/c/economics-and-business\"\n",
    "cyber = CyberScrapper(url, max_volume=2000, save_path=\"papers_6.csv\", num_page=6643)\n",
    "cyber.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7407742c6b58b0d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:04:20.104151845Z",
     "start_time": "2023-09-13T12:04:19.821170290Z"
    }
   },
   "outputs": [],
   "source": [
    "cyber.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
