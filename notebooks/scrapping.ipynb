{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44590bb828b674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium and chromium must be pre-installed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import traceback\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from fake_useragent import UserAgent\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5b22f",
   "metadata": {},
   "source": [
    "### RBC.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859e390f2a03c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://www.rbc.ru/finances/?utm_source=topline\", \"https://www.rbc.ru/economics/?utm_source=topline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41ba2b7dc77605fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T15:24:13.668507037Z",
     "start_time": "2023-09-09T15:24:13.614678690Z"
    }
   },
   "outputs": [],
   "source": [
    "class Scrapper:\n",
    "    \"\"\"\n",
    "    Collecting pages urls and texts from rbc.ru\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url, max_volume, save_path):\n",
    "        self.url = base_url\n",
    "        self.max_vol = max_volume\n",
    "        self.path = save_path\n",
    "        self.dtf = pd.DataFrame(columns=[\n",
    "            \"url\"\n",
    "        ])\n",
    "    \n",
    "    def get_urls(self):\n",
    "        num = self.max_vol\n",
    "        \n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(self.url)\n",
    "        \n",
    "        try:\n",
    "            popup = driver.find_element(By.CLASS_NAME, \"live-tv-popup__close\")\n",
    "            popup.click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        urls = []\n",
    "        \n",
    "        while num:\n",
    "            try:\n",
    "                objects = driver.find_elements(By.CLASS_NAME, 'item__link')\n",
    "                url = []\n",
    "                for obj in objects:\n",
    "                    urls.append(obj.get_attribute(\"href\"))\n",
    "                \n",
    "                num -= 1\n",
    "            except Exception as er:\n",
    "                print(er)\n",
    "                driver.quit()\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            print(f\"INFO collected {len(urls)} urls\")\n",
    "            sleep(9)\n",
    "        \n",
    "        self.dtf = pd.concat([self.dtf, pd.DataFrame(urls, columns=[\"url\"])], ignore_index=True)\n",
    "        \n",
    "        driver.quit()\n",
    "        self.save()\n",
    "        \n",
    "    def extract(self):\n",
    "        urls = pd.read_csv(self.path)\n",
    "        \n",
    "        driver = webdriver.Chrome()\n",
    "        texts = []\n",
    "        urls['text'] = None\n",
    "            \n",
    "        for i in range(urls.shape[0]):\n",
    "            driver.get(urls.loc[i][\"url\"])\n",
    "        \n",
    "            try:\n",
    "                popup = driver.find_element(By.CLASS_NAME, \"live-tv-popup__close\")\n",
    "                popup.click()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                text = ''\n",
    "                objects = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "                for obj in objects:\n",
    "                    text += obj.text\n",
    "                urls[\"text\"][i] = text\n",
    "                \n",
    "            except:\n",
    "                urls.to_csv(self.path)\n",
    "                driver.quit()\n",
    "            sleep(2)\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(f\"Collected {i} news\")\n",
    "        \n",
    "        urls.to_csv(self.path)\n",
    "        driver.quit()\n",
    "    \n",
    "    def save(self):\n",
    "        self.dtf.to_csv(self.path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f719ec2ce326626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T15:26:14.334539448Z",
     "start_time": "2023-09-09T15:24:25.305784375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO collected 20 urls\n",
      "INFO collected 52 urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58433/1825547926.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  urls[\"text\"][i] = text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 0 news\n",
      "Collected 50 news\n"
     ]
    }
   ],
   "source": [
    "sc = Scrapper(\"https://www.rbc.ru/finances/?utm_source=topline\", max_volume=2, save_path=\"test.csv\")\n",
    "sc.get_urls()\n",
    "sc.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d874417b147e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = Scrapper(urls[0], max_volume=1000, save_path=\"finance.scv\")\n",
    "bis = Scrapper(urls[1], max_volume=1000, save_path=\"business.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbb716b52a2142b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO collected 20 urls\n",
      "INFO collected 52 urls\n",
      "INFO collected 96 urls\n",
      "INFO collected 152 urls\n",
      "INFO collected 220 urls\n",
      "INFO collected 300 urls\n",
      "INFO collected 392 urls\n",
      "INFO collected 496 urls\n",
      "INFO collected 612 urls\n",
      "INFO collected 740 urls\n",
      "INFO collected 880 urls\n",
      "INFO collected 1032 urls\n",
      "INFO collected 1196 urls\n",
      "INFO collected 1372 urls\n",
      "INFO collected 1560 urls\n",
      "INFO collected 1760 urls\n",
      "INFO collected 1960 urls\n",
      "INFO collected 2160 urls\n",
      "INFO collected 2360 urls\n",
      "INFO collected 2560 urls\n",
      "INFO collected 2760 urls\n",
      "INFO collected 2960 urls\n",
      "INFO collected 3160 urls\n",
      "INFO collected 3360 urls\n",
      "INFO collected 3560 urls\n",
      "INFO collected 3760 urls\n",
      "INFO collected 3960 urls\n",
      "INFO collected 4160 urls\n",
      "INFO collected 4360 urls\n",
      "INFO collected 4560 urls\n",
      "INFO collected 4760 urls\n",
      "INFO collected 4960 urls\n",
      "INFO collected 5160 urls\n",
      "INFO collected 5360 urls\n",
      "INFO collected 5560 urls\n",
      "INFO collected 5760 urls\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m fin\u001b[38;5;241m.\u001b[39mextract()\n",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m, in \u001b[0;36mScrapper.get_urls\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m     driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollTo(0, document.body.scrollHeight);\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINFO collected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(urls)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m urls\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtf, pd\u001b[38;5;241m.\u001b[39mDataFrame(urls, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m])], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fin.get_urls()\n",
    "fin.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae815c76eb1d19a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bis.get_urls()\n",
    "bis.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc99a4e5c244d4",
   "metadata": {},
   "source": [
    "### CyberLeninka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311f9de74c7b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyberScrapper:\n",
    "    \"\"\"\n",
    "    Collecting pages urls and texts from cyberleninka.ru\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url, max_volume, save_path, num_page=None):\n",
    "        self.url = base_url\n",
    "        self.max_vol = max_volume\n",
    "        self.path = save_path\n",
    "        self.num_page = num_page\n",
    "        self.columns = [\"url\", \"author\", \"title\", \"text\", \"year\", \"labels\", \"views\", \n",
    "                                          \"downloads\", \"likes\", \"dislikes\", \"journal\"]\n",
    "        self.data = pd.DataFrame(columns=self.columns)\n",
    "        \n",
    "    def get(self):\n",
    "        \n",
    "        ua = UserAgent()\n",
    "        user_agent = ua.random\n",
    "        \n",
    "        driver = webdriver.Chrome()\n",
    "        \n",
    "        driver.execute_cdp_cmd(\"Network.setUserAgentOverride\", {\"userAgent\": user_agent})\n",
    "        if self.num_page:\n",
    "            driver.get(self.url + f\"/{self.num_page}\")\n",
    "        else:\n",
    "            driver.get(self.url)\n",
    "        num = self.max_vol\n",
    "        \n",
    "        # num of li elements on the page\n",
    "        last_paper_on_page = -7\n",
    "        if self.num_page:\n",
    "            page_num = self.num_page\n",
    "        else:\n",
    "            page_num = 2\n",
    "        \n",
    "        try:\n",
    "            while num:\n",
    "                \n",
    "                print(f\"Papers {self.data.shape[0]} saved\")\n",
    "                elements = driver.find_elements(By.TAG_NAME, \"li\")\n",
    "                articles = elements[:last_paper_on_page]\n",
    "                next_page = self.url + f\"/{page_num}\"\n",
    "                \n",
    "                for article in articles:\n",
    "                    \n",
    "                    num -= 1\n",
    "                    href = article.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\") \n",
    "                    driver.get(href)\n",
    "                    \n",
    "                    objects = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "                    \n",
    "                    # get text of paper\n",
    "                    text = ''\n",
    "                    for obj in objects:\n",
    "                        text += obj.text\n",
    "                    \n",
    "                    # author\n",
    "                    try:\n",
    "                        author = driver.find_element(By.CLASS_NAME, \"hl\").text\n",
    "                    except:\n",
    "                        author = None\n",
    "                    try:\n",
    "                        views = driver.find_element(By.CLASS_NAME, \"statitem.views\").text\n",
    "                    except:\n",
    "                        views = None\n",
    "                    try:\n",
    "                        down = driver.find_element(By.CLASS_NAME, \"statitem.downloads\").text\n",
    "                    except:\n",
    "                        down = None\n",
    "                    try:\n",
    "                        likes = driver.find_element(By.CLASS_NAME, \"likes\").text.split(\"\\n\")\n",
    "                    except:\n",
    "                        likes = [None, None]\n",
    "                    try:\n",
    "                        year = driver.find_element(By.CLASS_NAME, \"label.year\").find_element(By.TAG_NAME, \"time\").text\n",
    "                    except:\n",
    "                        year = None\n",
    "                    try:    \n",
    "                        journal = driver.find_element(By.CLASS_NAME, \"half\").find_elements(By.TAG_NAME, \"a\")[-1].text\n",
    "                    except:\n",
    "                        journal = None\n",
    "                    try:\n",
    "                        words = [i.text for i in driver.find_element(By.CLASS_NAME, \"full.keywords\").find_elements(By.CLASS_NAME, \"hl.to-search\")]\n",
    "                    except:\n",
    "                        words = None\n",
    "                    try:\n",
    "                        title = driver.find_element(By.TAG_NAME, \"i\").text\n",
    "                    except:\n",
    "                        title = None\n",
    "                    \n",
    "                    lst = [(href, \n",
    "                            author, \n",
    "                            title,\n",
    "                            text, \n",
    "                            year, \n",
    "                            words, \n",
    "                            views, \n",
    "                            down, \n",
    "                            likes[0], \n",
    "                            likes[1], \n",
    "                            journal)]\n",
    "                    to_add = pd.DataFrame(lst, columns=self.columns)\n",
    "                    self.data = pd.concat([self.data, to_add], ignore_index=True)\n",
    "                    sleep(5)\n",
    "                    driver.back()\n",
    "                \n",
    "                # Change UA \n",
    "                driver.execute_cdp_cmd(\"Network.setUserAgentOverride\", {\"userAgent\": ua.random})\n",
    "                driver.get(next_page)\n",
    "                page_num += 1\n",
    "                \n",
    "        except Exception as ex:\n",
    "            print(\"Last page:\", page_num)\n",
    "            traceback.print_exc()\n",
    "            driver.quit()\n",
    "            \n",
    "        print(\"Last page:\", page_num)\n",
    "        return self.data\n",
    "    \n",
    "    def save(self):\n",
    "        self.data.to_csv(self.path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126f7a9b65dd8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cyberleninka.ru/article/c/economics-and-business\"\n",
    "cyber = CyberScrapper(url, max_volume=2000, save_path=\"papers_5.csv\", num_page=6642)\n",
    "cyber.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7407742c6b58b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyber.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
